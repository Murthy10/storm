\begin{savequote}[75mm]
The man who is swimming against the stream knows the strength of it.
\qauthor{Woodrow Wilson}
\end{savequote}

\chapter{Stream Processing}
Steams are older than computers so it is not a big surprise that streams
and the processing of them isn't a absolutly new topic in the computer science world.
Historically there are techniques like \textbf{logging} or in the domain driven development area there is \textbf{event sourcing},
which are very similar to stream processing.
But they have not been always as important as they are today with the unbelivable amount of data involved.
The old way to handle the streams was an event driven one and to do analytics after storing the data.\\


\section{Initial example}
At this point I'd like to give you a small example of how streams and stream processing could be more and more
relevant. And I would like to do this by the example of a factory producing delicious frosted yoghurt.

\newpage

\subsection{Frozen yogurt factory}
Let me explain the \textit{old} event driven way with a small example.\\
Imagine a factory which produces frozen yogurt in different flavors.
They weigh and register every cup of yogurt at the end of the assembly line.\\

\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.4\textwidth]{images/cups.png}
\caption[Frozen yogurt assembly line]{frozen yogurt assembly line}
\end{figure}

\subsection{Common architecture}
The factory has sensors which weigh the cups and the weights are sent to a server.
The server handles the request and stores the frozen yogurt with his weight in the database.
For analytic tasks there is a web application. If you are now interested in the total amount of produced cups.
You can simply open the browser go to the analytic page.
Which starts a request to the server and the server will call the database with a quey like "select count(*) from frozen\_yogurt".
After the quey is executed you will get the result from the server and have the aggregated number on your screen.\\
(This is more or less a default example of a Three-tier architecture)

\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.5\textwidth]{images/three_tier.png}
\caption[Three-tier architecture]{Three-tier architecture}
\end{figure}

\newpage

Now the owner of the factory does a very good business and they are able to expand the production.
And with the expansion they add a lot more of sensors to the assembly line, like a temperature sensor,
optical recognition to check if the cups are always full and a lot more.\\
The requirements of the system are updated too, the owner wants to have statistical information about the production all the time
and immediately notifications if for example the temperature is to high.\\
These new requirements leads to new challenges in the architecutre of the system and are hard to implement with the
current state and the enormus data produced by all the sensors.

\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.5\textwidth]{images/sensors.png}
\caption[Sensors everywhere]{Sensors everywhere}
\end{figure}

\subsection{Streaming architecture}
A good way to handle this new requirements is to continuously aggregate and filter the stream of data before gets stored in the database.
For this purpose there has grown up a lot of new techniques and frameworks during the last few years.

\begin{figure}[H]
\centering
\captionsetup{justification=centering}
\includegraphics[width=0.7\textwidth]{images/stream.png}
\caption[streaming architecture]{streaming architecture}
\end{figure}

Instead of the old three-tier architecture they added a message / logging queue like Apache Kafka to handle all concurrent access to data.
Which enables a seamless access for the stream processing frameworks and the storing of the data for later analytics.

\subsection{Conclusion}
The new streaming architecture of the frozen yogurt factory has got a lot of advantages, but some big difficulties to handle too.
Now it is possible to get very fast access to production statistical information, deal with the huge amount of data and throw alerts in real time.
It's also helpfull in the developers perspective, because you can add several consumers to the stream.
Normaly these streams are immutable, which solves the concurrent access of different consumers.
Thus the requirements in such real time stream processing systems are ambitious and we will face them in the next section.

\newpage

\section{Eight Rules For Stream Procesing}
In the paper "The 8 Requirements of Real-Time Stream Processing" Michael Stonebraker, Ugur Cetintemel and Stan Zdonik
brought their thoughts about real-time stream processing together and wrote it down. It faces the fundamental ideas of stream processing
and sums them up in eight rules which every stream processing engine should require.
That allows us to measure and compare different technologies.
Thus leads me to take a closer look at these eight rules and add my personal reflection to them.

\subsection{Rule 1: Keep the Data Moving}
\textit{The first requirement for a real-time stream processing
        system is to process messages “in-stream”, without any
        requirement to store them to perform any operation or
        sequence of operations. Ideally the system should also use
        an active (i.e., non-polling) processing model.}

\medskip
This is a very important point for me too. The stream processing should not slow down the "in-stream".
How can this target be achieved? The paper raises the point of beeing  aware of expensive storage operation.
But I could imagine that this question depends most on the architecture of the processing engine.
A solution from my point of view could be to handle the streams as immutable and decouple the processing from the data stream sink.


\subsection{Rule 2: Query using SQL on Streams (StreamSQL)}
\textit{The second requirement is to support a high-level
        “StreamSQL” language with built-in extensible stream-
        oriented primitives and operators.}

\medskip
Use something like "StreamSQL" is not a must have requirement in my eyes.
I would even go further and say that a higher-level query language could not always fit the fast changing streams.
So a flexible and easy to expand query language would be a better choice for me.
Or if the maintainer are developers to realise the queries in code would be a good way too.
Even in spite of the heavy maintenance and intelligibility.

\newpage
\subsection{Rule 3: Handle Stream Imperfections (Delayed, Missing
and Out-of-Order Data)}
\textit{The third requirement is to have built-in mechanisms to
        provide resiliency against stream “imperfections”,
        including missing and out-of-order data, which are
        commonly present in real-world data streams.}

\medskip
If the data of the stream does not appear like it used to be the stream processing engine should not crash and handle the problem.
For me an other very fundamental requirement in the point of view that such streams often come from distributed systems.
Which leads to a lot of hard to control error sources.

\subsection{Rule 4: Generate Predictable Outcomes}
\textit{The fourth requirement is that a stream processing engine
        must guarant predictable and repeatable outcomes.}

\medskip
The same input should always result in the same output.
Easy said and easy to understand rule but in processing huge amount of data it could be in some cases very difficult.
Imagine that there is more data then you are able to handle with all your computing power,
but you still want to be able to process data and not every entry in the stream is absolutely important.
Thus it could be a good strategy to more or less randomly throw some messages away.
Nevertheless the decisions you do and the processing on it should be repeatable.

\subsection{Rule 5: Integrate Stored and Streaming Data}
\textit{The fifth requirement is to have the capability to efficiently
        store, access, and modify state information, and combine it
        with live streaming data. For seamless integration, the
        system should use a uniform language when dealing with
        either type of data.}

\medskip
I agree with this rule, past data and also current data should possible haven an influence
on the results of the processing. So it is a good idea to use a uniform language.
But I also think that the streaming engine needs to have the ability and flexibility to deal with different
and unexpected data.

\newpage
\subsection{Rule 6: Guarantee Data Safety and Availability}
\textit{The sixth requirement is to ensure that the applications are
        up and available, and the integrity of the data maintained at
        all times, despite failures.}

\medskip
Of course streaming engines need to be high-availability.
But this is a very hard deal in the perspective of high performance,
because high-availability leads to some inevitable overheads.
Thus I would say that it should be possible to handle guarante integrity of mission-critical information
and also support less critical information to be processed in a super performant but less integrity way.

\subsection{Rule 7: Partition and Scale Applications Automatically}
\textit{The seventh requirement is to have the capability to
        distribute processing across multiple processors and
        machines to achieve incremental scalability. Ideally, the
        distribution should be automatic and transparent.}

\medskip
In the case of huge amount of data the ability to scale vertically is an absolute must have feature of
every stream processing engine. For me this is one of the most important rules.

\subsection{Rule 8: Process and Respond Instantaneously}
\textit{The eighth requirement is that a stream processing system
must have a highly-optimized, minimal-overhead execution
engine to deliver real-time response for high-volume
applications.}

\medskip
Like at the rule 7 sad, this is a hard tradeoff between availability and integrity.
Thus a stream processing engine has to deal with this two requirements as good as possible
and find a best fitting solution.
Because the possible throughput can also have a decisive impact on the choice of a technology.




























